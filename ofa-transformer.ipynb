{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":45917,"databundleVersionId":5024308,"sourceType":"competition"},{"sourceId":4988409,"sourceType":"datasetVersion","datasetId":2893282},{"sourceId":8425503,"sourceType":"datasetVersion","datasetId":5016845},{"sourceId":119507000,"sourceType":"kernelVersion"}],"dockerImageVersionId":30388,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Captioning with OFA 🖼️📄\n*Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework*. Currently on of the SOTA in Image Captioning on COCO benchmark.\n\nHere, we perform image captioning with pretrained OFA on the test images. Check out their [github repo](https://github.com/OFA-Sys/OFA) so that you can explore further, possibly training too. The model is a generic multi modal framework, which means a lot of tasks can be accomplished by it. Hence we infer our captions with the help of a question prompt asking the model to describe the image.","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:59:55.284101Z","iopub.execute_input":"2023-02-19T02:59:55.284528Z","iopub.status.idle":"2023-02-19T03:00:31.17307Z","shell.execute_reply.started":"2023-02-19T02:59:55.284445Z","shell.execute_reply":"2023-02-19T03:00:31.171889Z"}}},{"cell_type":"code","source":"# Using the pre compiled wheel since we don't have internet on submission\n!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl","metadata":{"execution":{"iopub.status.busy":"2024-05-17T02:30:49.556056Z","iopub.execute_input":"2024-05-17T02:30:49.556511Z","iopub.status.idle":"2024-05-17T02:31:29.785276Z","shell.execute_reply.started":"2024-05-17T02:30:49.556419Z","shell.execute_reply":"2024-05-17T02:31:29.783551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport sys\nimport glob\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nimport torch\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom transformers import OFATokenizer, OFAModel\nfrom transformers.models.ofa.generate import sequence_generator","metadata":{"execution":{"iopub.status.busy":"2024-05-17T02:31:29.788196Z","iopub.execute_input":"2024-05-17T02:31:29.788684Z","iopub.status.idle":"2024-05-17T02:31:32.309303Z","shell.execute_reply.started":"2024-05-17T02:31:29.788637Z","shell.execute_reply":"2024-05-17T02:31:32.308206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"CKPT_DIR = \"/kaggle/input/stable-diffusion-data/OFA-large-caption/\"\nIMAGE_DIR = \"/kaggle/input/stable-diffusion-image-to-prompts/images\"\n\nBATCH_SIZE = 24","metadata":{"execution":{"iopub.status.busy":"2024-05-17T02:31:32.310782Z","iopub.execute_input":"2024-05-17T02:31:32.312030Z","iopub.status.idle":"2024-05-17T02:31:32.317741Z","shell.execute_reply.started":"2024-05-17T02:31:32.311984Z","shell.execute_reply":"2024-05-17T02:31:32.316186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading the pretrained OFA model ","metadata":{}},{"cell_type":"code","source":"mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\nresolution = 480\npatch_resize_transform = transforms.Compose([\n        lambda image: image.convert(\"RGB\"),\n        transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n        transforms.ToTensor(), \n        transforms.Normalize(mean=mean, std=std)\n    ])\n\ntokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\nmodel = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\ntxt = \" what does the image describe?\"\ninputs = tokenizer([txt], return_tensors=\"pt\").input_ids","metadata":{"execution":{"iopub.status.busy":"2024-05-12T22:14:10.766539Z","iopub.execute_input":"2024-05-12T22:14:10.767365Z","iopub.status.idle":"2024-05-12T22:14:37.155537Z","shell.execute_reply.started":"2024-05-12T22:14:10.767328Z","shell.execute_reply":"2024-05-12T22:14:37.154573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model EDA\nLets see how the model is performing on the sample images!","metadata":{}},{"cell_type":"code","source":"sample_images = glob.glob(\"/kaggle/input/stable-diffusion-image-to-prompts/images/*\")[:7]\nfig, ax = plt.subplots(7,1, figsize=(4,35))\n\nfor i,impath in enumerate(sample_images):\n    image = Image.open(impath)\n    image_t = patch_resize_transform(image).cuda().unsqueeze(0)\n    out = model.generate(inputs.cuda(), patch_images=image_t.cuda(), num_beams=5, no_repeat_ngram_size=2)\n    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n    ax[i].imshow(image)\n    ax[i].text(1.1, .5, out_captions[0], horizontalalignment='left', verticalalignment='center', transform=ax[i].transAxes)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-12T22:15:09.726112Z","iopub.execute_input":"2024-05-12T22:15:09.726866Z","iopub.status.idle":"2024-05-12T22:15:30.127595Z","shell.execute_reply.started":"2024-05-12T22:15:09.726824Z","shell.execute_reply":"2024-05-12T22:15:30.125817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"Loading the embedding model required for submission, as provided by the organizers.","metadata":{}},{"cell_type":"code","source":"sys.path.append('../input/sentence-transformers-222/sentence-transformers')\nfrom sentence_transformers import SentenceTransformer, models\n\ncomp_path = Path('../input/stable-diffusion-image-to-prompts/')\nst_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')","metadata":{"execution":{"iopub.status.busy":"2024-05-12T22:16:09.791310Z","iopub.execute_input":"2024-05-12T22:16:09.791982Z","iopub.status.idle":"2024-05-12T22:16:12.747569Z","shell.execute_reply.started":"2024-05-12T22:16:09.791946Z","shell.execute_reply":"2024-05-12T22:16:12.746687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A simple Image Loader to make our life easier!","metadata":{}},{"cell_type":"code","source":"class ImageGen(Dataset):\n    def __init__(self, root, batch_size=32):\n        self.root = root\n        self.im_paths = os.listdir(self.root)\n        self.batch_size = batch_size\n        self.sz = len(self.im_paths)\n        self.genlen = self.sz//self.batch_size + int(self.sz%self.batch_size > 0)\n        \n    def __getitem__(self, index):\n        if index >= self.genlen:\n            raise IndexError(\"Out of bounds\")\n        \n        l, r = index*self.batch_size, min(self.sz, (index+1)*self.batch_size)\n        \n        f_paths = [os.path.join(self.root, self.im_paths[i]) for i in range(l,r)]\n        f_ids = [self.im_paths[i][:-4] for i in range(l,r)]\n        \n        ims = [Image.open(f_path) for f_path in f_paths]\n        ims = [patch_resize_transform(im).cuda().unsqueeze(0) for im in ims]\n        ims = torch.cat(ims)\n        \n        return ims, f_ids\n    \n    def __len__(self):\n        return self.genlen\n        ","metadata":{"execution":{"iopub.status.busy":"2024-05-12T22:16:12.749335Z","iopub.execute_input":"2024-05-12T22:16:12.749637Z","iopub.status.idle":"2024-05-12T22:16:12.761186Z","shell.execute_reply.started":"2024-05-12T22:16:12.749609Z","shell.execute_reply":"2024-05-12T22:16:12.760099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inference loop! Here we also utilize the idea from https://www.kaggle.com/code/yawata/post-processing-adding-modifiers to add modifiers to the output caption.","metadata":{}},{"cell_type":"code","source":"sub_ids = []\nsub_embeds = []\n\nimgen = ImageGen(IMAGE_DIR, BATCH_SIZE)\n\nfor b in imgen:\n    for j in range(len(b[1])):\n        sub_ids.extend([f\"{b[1][j]}_{i}\" for i in range(384)])\n    \n    img_batch = b[0]\n    out = model.generate(inputs.repeat(len(img_batch), 1).cuda(), patch_images=img_batch, num_beams=5, no_repeat_ngram_size=2)\n    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n    out_captions = [cap + \", fine details, masterpiece\" for cap in out_captions]\n    \n    embeddings = st_model.encode(out_captions).flatten()\n    sub_embeds.extend(embeddings)","metadata":{"execution":{"iopub.status.busy":"2024-05-12T22:16:12.762587Z","iopub.execute_input":"2024-05-12T22:16:12.763692Z","iopub.status.idle":"2024-05-12T22:16:19.235905Z","shell.execute_reply.started":"2024-05-12T22:16:12.763652Z","shell.execute_reply":"2024-05-12T22:16:19.235004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame({\"imgId_eId\": sub_ids, \"val\": sub_embeds})\nprint(sub.shape)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-12T22:16:19.237658Z","iopub.execute_input":"2024-05-12T22:16:19.237934Z","iopub.status.idle":"2024-05-12T22:16:19.257944Z","shell.execute_reply.started":"2024-05-12T22:16:19.237908Z","shell.execute_reply":"2024-05-12T22:16:19.257003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-20T22:03:03.656698Z","iopub.execute_input":"2023-02-20T22:03:03.657457Z","iopub.status.idle":"2023-02-20T22:03:03.669616Z","shell.execute_reply.started":"2023-02-20T22:03:03.657422Z","shell.execute_reply":"2023-02-20T22:03:03.668791Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
